{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Current working directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq\n",
      "--- Reading from default.yml to build up initial hyperparameters dictionary...\n",
      "--- Checking to see if we want to override default hyperparameters with new file...\n",
      "--- Override file found! Updating with hyperparameters from default.yml...\n",
      "--- Hyperparameters/metadata set as follows (may be altered later if using EXISTING wandb sweep):\n",
      "{'entity': 'jcgraciosa',\n",
      " 'method': 'grid',\n",
      " 'parameters': {'activation_function': {'value': 'relu'},\n",
      "                'batch_normalisation': {'value': False},\n",
      "                'batch_size': {'value': 16},\n",
      "                'cat_scaling': {'value': 1},\n",
      "                'categorical_output': {'value': True},\n",
      "                'dataset': {'value': 'lin_samp_50'},\n",
      "                'dropout': {'value': 0.2},\n",
      "                'embeddings': {'value': 0},\n",
      "                'epochs': {'value': 120},\n",
      "                'exclude_file': {'value': 'xyz.csv'},\n",
      "                'hidden_layers': {'value': [500, 100]},\n",
      "                'k_folds': {'value': 5},\n",
      "                'kernel_size': {'value': [1, 1]},\n",
      "                'label_smooth': {'value': False},\n",
      "                'learning_rate': {'value': 0.01},\n",
      "                'mw_cats': {'value': [0, 0.33, 0.65, 1.0]},\n",
      "                'parameter_set': {'value': 'default'},\n",
      "                'protect_great': {'value': False},\n",
      "                'regr_scaling': {'value': 0},\n",
      "                'regression_output': {'value': False},\n",
      "                'sampling_weights': {'value': [0.33, 0.33, 0.33]},\n",
      "                'target': {'value': 'MR_GCMT'},\n",
      "                'use_SMOTE': {'value': False},\n",
      "                'use_k_fold': {'value': False},\n",
      "                'use_random_sampling': {'value': False},\n",
      "                'weight_decay': {'value': 0}},\n",
      " 'project': 'ml4szeq'}\n"
     ]
    }
   ],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %%\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "from captum.attr import LRP \n",
    "from tqdm.std import tqdm\n",
    "\n",
    "# for the LRP\n",
    "import torch.nn as nn\n",
    "from captum.attr import InputXGradient, LRP, IntegratedGradients\n",
    "from captum.attr._utils.lrp_rules import (\n",
    "    Alpha1_Beta0_Rule,\n",
    "    EpsilonRule,\n",
    "    GammaRule,\n",
    "    IdentityRule,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import vis_pkg\n",
    "import helper_pkg\n",
    "import json\n",
    "import cartopy as cartopy\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import cmocean\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['hatch.linewidth'] = 0.3\n",
    "mpl.rcParams.update({'hatch.color': 'gray'})\n",
    "\n",
    "# Terrible hack to make sure Jupyter notebooks (which use different PYTHONPATH\n",
    "# for some reason!) actually sees src/ directory so we can import from there.\n",
    "os.chdir(\"/Users/jgra0019/Documents/codes/ml4szeq/ml4szeq\")\n",
    "print(f\"--- Current working directory: {os.getcwd()}\")\n",
    "if not any([re.search(\"src$\", path) for path in sys.path]):\n",
    "    sys.path.append(str(Path.cwd() / \"src\"))\n",
    "\n",
    "import default\n",
    "from dataset import DFDataset\n",
    "from fit import Fit\n",
    "from utils import (convert_hyperparam_config_to_values, get_config,\n",
    "                   get_full_hyperparam_config, load_data)\n",
    "from model import *\n",
    "from predictor import predictions\n",
    "\n",
    "# %% GETTING HYPER-PARAMETERS\n",
    "config_override_file = get_config(\"PARAMETER_YAML_FILE\", None)\n",
    "hyperparam_config = get_full_hyperparam_config(config_override_file=config_override_file)\n",
    "print(\n",
    "    \"--- Hyperparameters/metadata set as follows (may be altered later if using EXISTING wandb sweep):\"\n",
    ")\n",
    "pprint(hyperparam_config)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping code\n",
    "All data are used in training, portion is reserved for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/model4\n",
      "Output directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/xai-map-model4-cls3\n"
     ]
    }
   ],
   "source": [
    "''' settings to define '''\n",
    "\n",
    "tr_all_region_N = 4\n",
    "sep_dist = 100\n",
    "mod_rank = 1 # rank of the model - valid numbers are 1, 2, 3, 4, 5\n",
    "\n",
    "# for the model_list, list the part with hyperparameter list\n",
    "num_class = 3\n",
    "num_model = 1\n",
    "device = \"cpu\"\n",
    "\n",
    "# other important config\n",
    "tr_half_use = None\n",
    "do_tr_all_region = True\n",
    "\n",
    "#scenario = 4 # 3 - with RAND_CTRL; 4 - without RAND_CTRL\n",
    "do_stnd = True  # perform standardization of the relevance values or not\n",
    "use_tp = True   # use True positive \n",
    "apply_thresh = False    # set to True if we apply threshold to heatmap values\n",
    "\n",
    "thresh_val = 0\n",
    "\n",
    "model_dir = f\"/Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls{num_class}\"\n",
    "if num_class == 2:\n",
    "    hparam_file = f\"{model_dir}/model{tr_all_region_N}/model{tr_all_region_N}-sep{sep_dist}-top10.json\"\n",
    "elif num_class == 3:\n",
    "    hparam_file = f\"{model_dir}/model{tr_all_region_N}/model{tr_all_region_N}-sep{sep_dist}-cls{num_class}-top10.json\"\n",
    "hparam_file = Path(hparam_file)\n",
    "\n",
    "with open(hparam_file) as json_data_file:\n",
    "    hparam = json.load(json_data_file)\n",
    "\n",
    "hparam_sset = hparam\n",
    "\n",
    "hparam_sset = hparam_sset[str(mod_rank)]\n",
    "\n",
    "epoch_use = hparam_sset[\"epoch_use\"]\n",
    "folder_use = hparam_sset[\"folder\"]\n",
    "\n",
    "# xai parameters \n",
    "algo_use = \"lrp_def\" # use default\n",
    "\n",
    "# FIXME: set these correctly\n",
    "out_dir = Path(f\"{model_dir}/xai-map-model{tr_all_region_N}-cls{num_class}\")\n",
    "model_dir = Path(f\"{model_dir}/model{tr_all_region_N}\")\n",
    "\n",
    "print(f\"Model directory: {model_dir}\")\n",
    "print(f\"Output directory: {out_dir}\")\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "# plotting limits - only for the sep in [50, 100]\n",
    "if num_class == 2:\n",
    "    if sep_dist == 50:\n",
    "        vminmax = 3\n",
    "        histminmax = 3.3\n",
    "        tickmarks = [-3, -2, -1, 0, 1, 2, 3]\n",
    "        ymax = 15\n",
    "    elif sep_dist == 100:\n",
    "        vminmax = 3\n",
    "        histminmax = 5.6\n",
    "        tickmarks = [-3, -2, -1, 0, 1, 2, 3]\n",
    "        ymax = 15\n",
    "elif num_class == 3:\n",
    "    if sep_dist == 50:\n",
    "        vminmax = 3 \n",
    "        histminmax = 5\n",
    "        tickmarks = [-3, -2, -1, 0, 1, 2, 3]\n",
    "        ymax = 15\n",
    "    elif sep_dist == 100:\n",
    "        vminmax = 3 \n",
    "        histminmax = 5\n",
    "        tickmarks = [-3, -2, -1, 0, 1, 2, 3]\n",
    "        ymax = 15\n",
    "\n",
    "tickmarks = np.array(tickmarks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Preparations for the map and function declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function declarations '''\n",
    "\n",
    "# arrange features properly\n",
    "def arrange_features(feat_list):\n",
    "    phys_state_list = [\"CRD_UP\", \"CRS_UP\", \"CRM_UP\",\n",
    "                   \"INV_UP\", \"DLT_UP\", \"SED\", \"SRO\", \"IRO\", \"LRO\"\n",
    "                   ]\n",
    "    dyna_list = [\"FRE_DG\", \"FRE_UP\", \"BGR_DG\",  \n",
    "                \"EGO_UP\", \n",
    "                \"EGO_L_UP\", \n",
    "                \"EGO_SL_UP\",  \n",
    "                \"EGO_UM_UP\",\n",
    "                \"EGR_DG\", \n",
    "                \"EGR_UP\", \n",
    "                \"EGR_BG_UP\",\n",
    "                \"DXT\", \"FDM\", \"SDM\"]\n",
    "    kine_list = [\"V_UP\", \"V_TN\", \"AGE\"]\n",
    "    \n",
    "    new_feat_list = []\n",
    "\n",
    "    # loop through all phys state list\n",
    "    for phys_state in phys_state_list:\n",
    "        for feat in feat_list:\n",
    "            if (phys_state in feat) and (feat not in new_feat_list):\n",
    "                new_feat_list.append(feat)\n",
    "            \n",
    "    # loop through all dyna list\n",
    "    for dyna in dyna_list:\n",
    "        for feat in feat_list:\n",
    "            if (dyna in feat) and (feat not in new_feat_list):\n",
    "                new_feat_list.append(feat)\n",
    "    \n",
    "    # loop through all kinematic state list\n",
    "    for kine in kine_list:\n",
    "        for feat in feat_list:\n",
    "            if (kine in feat) and (feat not in new_feat_list):\n",
    "                new_feat_list.append(feat)\n",
    "\n",
    "    # if scenario == 3:\n",
    "    #     new_feat_list = new_feat_list + [\"TRG_STD1\", \"TRG_STD2\", \"TRG_STD3\", \"TRG_STD4\", \"TRG_STD5\", \"RND_CTRL\"]\n",
    "    # else:\n",
    "    #     new_feat_list.append(\"RND_CTRL\")\n",
    "\n",
    "    return new_feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Preparations for XAI plot'''\n",
    "\n",
    "xtick_lab = [\"Curvature (along-dip)\", \n",
    "            \"Curvature (along-strike)\", \n",
    "            \"Curvature (mean)\",\n",
    "            \"2nd strain inv. (UP)\", \n",
    "            \"Dilatation (UP)\",   \n",
    "            \"Sediment thickness\", \n",
    "            r\"Roughness (short $\\lambda$)\",\n",
    "            r\"Roughness (intermediate $\\lambda$)\", \n",
    "            r\"Roughness (long $\\lambda$)\",  \n",
    "            \"Free air gravity anomaly (DG)\",\n",
    "            \"Free air gravity anomaly (UP)\",\n",
    "            #\"Bouguer gravity anomaly (UP)\", # is actually DG\n",
    "            \"EGM 2008 geoid (UP)\",\n",
    "            \"EGM 2008 geoid L (UP)\",\n",
    "            \"EGM 2008 geoid SL (UP)\",\n",
    "            \"EGM 2008 geoid UM (UP)\",\n",
    "            \"EGM 2008 Free air gravity anomaly (DG)\",\n",
    "            \"EGM 2008 Free air gravity anomaly (UP)\",\n",
    "            \"EGM 2008 Bouguer anomaly (UP)\",\n",
    "            \"Slab depth\", \n",
    "            \"Depth gradient (magnitude)\",\n",
    "            \"Depth curvature (magnitude)\",\n",
    "             \"Plate motion\"]\n",
    "            #\"Null model\"]\n",
    "\n",
    "borders = [ 2, 4, 6, 8, 10, 13, 15, 17, 19, 21, 23, 25, \n",
    "            27, 29, 31, 33, 35, 37, 40, 43, 46]\n",
    "            #51]\n",
    "\n",
    "# 2 rows each, except for the non-grid data\n",
    "# if scenario == 3:\n",
    "#     borders = borders + [52, 53, 54, 55, 56]\n",
    "#     xtick_lab = xtick_lab + [\"1 std\", \"2 std\", \"3 std\", \"4 std\", \"5 std\"]\n",
    "\n",
    "# create the colormap\n",
    "cmap_use = cmocean.cm.amp\n",
    "cmap = cmocean.tools.crop_by_percent(cmap_use, per = 10, which='max', N=None)\n",
    "\n",
    "if num_class == 2:\n",
    "    hmap_labels = [ r\"$M_w < 8.0$\",\n",
    "                    r\"$M_w \\geq 8.0$\"\n",
    "                ]\n",
    "elif num_class == 3:\n",
    "    hmap_labels = [ r\"$M_w < 6.4$\",\n",
    "                    r\"$6.4 \\leq M_w < 8.3$\",\n",
    "                    r\"$M_w \\geq 8.3$\"\n",
    "                ]\n",
    "\n",
    "if do_stnd:\n",
    "    bin_use = np.linspace(-3, 3)\n",
    "else:\n",
    "    bin_use = np.linspace(-5, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Loop through all the models, perform predictions, and map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' MACHINE LEARNING STUFF HERE '''\n",
    "random.seed(43) # set the random seed \n",
    "\n",
    "# set-up hyperparameters - override values in default.yml \n",
    "hyperparam_config[\"parameters\"][\"exclude_file\"][\"value\"] = \"xyz.csv\" \n",
    "hyperparam_config[\"parameters\"][\"dropout\"][\"value\"] = float(hparam_sset[\"dropout\"])\n",
    "hyperparam_config[\"parameters\"][\"hidden_layers\"][\"value\"] = hparam_sset['hidden_layers']\n",
    "hyperparam_config[\"parameters\"][\"batch_size\"][\"value\"] = hparam_sset[\"batch_sz\"]\n",
    "hyperparam_config[\"parameters\"][\"learning_rate\"][\"value\"] = float(hparam_sset[\"lr\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[400, 500]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparam_config[\"parameters\"][\"hidden_layers\"][\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping any columns which have fewer than  90% (531.9) values\n",
      "Dropping any rows which are missing *any* input variables\n",
      "Dropped 33 columns, and 35 rows.\n",
      "Executing power transformer ... \n",
      "Finished preprocessing! Number of features:  49\n",
      "Size of training/validation data:  (107, 496)\n",
      "Class 0: 39\n",
      "Class 1: 23\n",
      "Class 2: 45\n",
      "Original length: (556,)\n",
      "Train length: (107, 496)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (556). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params, _ = convert_hyperparam_config_to_values(hyperparam_config) # convert here to include whatever were overriden\n",
    "data_suffix = params.get(\"dataset\", \"16k\")  # which dataset you want as input\n",
    "data_folder = default.ROOT_DATA_DIRECTORY / data_suffix\n",
    "use_cache = False\n",
    "\n",
    "dpi = 300 # use small if still testing\n",
    "\n",
    "preprocessor = load_data(\n",
    "        data_folder=data_folder,\n",
    "        exclude_file=f\"xyz.csv\",\n",
    "        target=params[\"target\"],\n",
    "        cats=params[\"mw_cats\"],\n",
    "        rand_seed = None, # for sampling with replacement \n",
    "        kernel_size=params[\"kernel_size\"],\n",
    "        use_cache=use_cache,\n",
    "        rd_exclude = False if do_tr_all_region else True,\n",
    "        protect_great=params[\"protect_great\"],\n",
    "        tr_half_use = tr_half_use,\n",
    "        sep_dist = sep_dist,\n",
    "        tr_all_region = False if do_tr_all_region else None,\n",
    "        tr_all_region_N = tr_all_region_N\n",
    "    )\n",
    "\n",
    "print(\"Finished preprocessing! Number of features: \", len(preprocessor.inputs))\n",
    "print(\"Size of training/validation data: \", preprocessor.dataframe.shape)\n",
    "print(f\"Class 0: {(preprocessor.dataframe.MW_CAT == 0).sum()}\")\n",
    "print(f\"Class 1: {(preprocessor.dataframe.MW_CAT == 1).sum()}\")\n",
    "print(f\"Class 2: {(preprocessor.dataframe.MW_CAT == 2).sum()}\")\n",
    "\n",
    "''' some checks '''\n",
    "# check for data leakage\n",
    "print(f\"Original length: {preprocessor.cond_fin.shape}\")\n",
    "print(f\"Train length: {preprocessor.dataframe.shape}\")\n",
    "\n",
    "    # Define arguments to be passed into our testing loop function.\n",
    "full_pred_kwargs = dict(\n",
    "    df=preprocessor.dataframe,\n",
    "    inputs=preprocessor.inputs,\n",
    "    hyperparam_config=hyperparam_config,\n",
    "    model_name_add=None,\n",
    "    use_wandb=False,\n",
    ")\n",
    "\n",
    "# create pred_ds \n",
    "pred_ds = DFDataset(dataframe = preprocessor.dataframe, \n",
    "                    inputs=preprocessor.inputs, \n",
    "                    target=preprocessor.target, \n",
    "                    force_cats = 5)\n",
    "pred_dl = torch.utils.data.DataLoader(pred_ds, batch_size = 1, shuffle=False)\n",
    "test_df = copy.deepcopy(pred_dl.dataset.dataframe)\n",
    "\n",
    "out_df = {}\n",
    "out_df[\"S_AVE\"] = test_df[\"S_AVE\"]\n",
    "out_df[\"LON_AVE\"] = test_df[\"LON_AVE\"]\n",
    "out_df[\"LAT_AVE\"] = test_df[\"LAT_AVE\"]\n",
    "out_df[\"MR_ISC\"] = test_df[\"MR_ISC\"]\n",
    "out_df[\"MR_GCMT\"] = test_df[\"MR_GCMT\"]\n",
    "out_df[\"MW_CAT\"] = test_df[\"MW_CAT\"]\n",
    "\n",
    "################## PERFORM PREDICTION \n",
    "model_fname = \"epoch-\" + str(epoch_use) + \".pt\"\n",
    "idx = 0\n",
    "\n",
    "model_path = model_dir/hparam_sset[\"folder\"]\n",
    "    \n",
    "pred_obj = Fit(fit_on_init = False, **full_pred_kwargs, force_cats = 0) # initialize lang pirmi\n",
    "pred_model = copy.deepcopy(pred_obj.model)\n",
    "\n",
    "#loop through all \n",
    "pred_model.load_state_dict(torch.load(model_path))\n",
    "pred_model.to(device)\n",
    "\n",
    "pred_model.eval()\n",
    "preds = np.zeros([len(pred_dl.dataset.dataframe), num_class])\n",
    "class_preds = np.zeros([len(pred_dl.dataset.dataframe)])\n",
    "\n",
    "# Loop through test data\n",
    "with torch.no_grad():\n",
    "    for i, ((x_cont, x_region), (cat_labels, cont_labels)) in enumerate(pred_dl): \n",
    "        x_cont, x_region, cat_labels, cont_labels = (\n",
    "            x_cont.to(device),\n",
    "            x_region.to(device),\n",
    "            cat_labels.to(device),\n",
    "            cont_labels.to(device),\n",
    "        )\n",
    "\n",
    "        # Get outputs and calculate loss\n",
    "        cat = pred_model(x_cont)\n",
    "        pred_vals = torch.sigmoid(cat)\n",
    "        _, pred_vals2 = torch.max(torch.sigmoid(cat), 1)\n",
    "        #print(cat)\n",
    "\n",
    "        preds[i] = pred_vals\n",
    "        class_preds[i] = pred_vals2\n",
    "\n",
    "    # save to dataframe for evaluation\n",
    "    for i in range(num_class):\n",
    "        out_df[\"MDL_\"+ str(idx) + \"_CLS_\" + str(i)] = preds[:, i]\n",
    "\n",
    "out_df[\"MDL_\" + str(idx) + \"_PRED\"] = class_preds\n",
    "idx += 1\n",
    "\n",
    "out_df = pd.DataFrame(out_df)\n",
    "out_df[\"CLASS_PRED\"] = class_preds\n",
    "\n",
    "########## prepare the results\n",
    "num_model = 1\n",
    "\n",
    "col_list =[\"MDL_\" + str(x) + \"_CLS_0\" for x in range(num_model)]\n",
    "cls0_df = out_df[ [\"S_AVE\"] + col_list]\n",
    "cls0_df = cls0_df.assign(MEAN=cls0_df[col_list].mean(axis = 1))\n",
    "cls0_df = cls0_df.assign(STD=cls0_df[col_list].std(axis = 1))\n",
    "\n",
    "col_list = [\"MDL_\" + str(x) + \"_CLS_1\" for x in range(num_model)]\n",
    "cls1_df = out_df[[\"S_AVE\"] + col_list]\n",
    "cls1_df = cls1_df.assign(MEAN=cls1_df[col_list].mean(axis = 1))\n",
    "cls1_df = cls1_df.assign(STD=cls1_df[col_list].std(axis = 1))\n",
    "\n",
    "if num_class == 3:\n",
    "    col_list = [\"MDL_\" + str(x) + \"_CLS_2\" for x in range(num_model)]\n",
    "    cls2_df = out_df[[\"S_AVE\"] + col_list]\n",
    "    cls2_df = cls2_df.assign(MEAN=cls2_df[col_list].mean(axis = 1))\n",
    "    cls2_df = cls2_df.assign(STD=cls2_df[col_list].std(axis = 1))\n",
    "\n",
    "col_list = [\"MDL_\" + str(x) + \"_PRED\" for x in range(num_model)]\n",
    "pred_df = out_df[[\"S_AVE\"] + col_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45070423, 0.28915663, 0.26666667])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will return the f1-score for class 0, 1, 2\n",
    "f1_score(out_df[\"MW_CAT\"], out_df[\"CLASS_PRED\"], average = None) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 20,  3],\n",
       "       [ 7, 12,  4],\n",
       "       [ 9, 28,  8]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(out_df[\"MW_CAT\"], out_df[\"CLASS_PRED\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "''' SECTION ON XAI '''\n",
    "model_name = f\"model{tr_all_region_N}-sep{sep_dist}-rank{mod_rank}-cls{num_class}\"\n",
    "pred_obj = Fit(fit_on_init = False, **full_pred_kwargs, force_cats = 0) # initialize lang pirm\n",
    "pred_model = copy.deepcopy(pred_obj.model)\n",
    "pred_model.load_state_dict(torch.load(model_path))\n",
    "pred_model.to(device)\n",
    "pred_model.eval()\n",
    "\n",
    "# for int_grad \n",
    "attr_obj = LRP(pred_model)\n",
    "\n",
    "# create the containers of the attribution value and convergence delta\n",
    "c0_attr = None\n",
    "c1_attr = None\n",
    "c2_attr = None\n",
    "\n",
    "c0_delta = None\n",
    "c1_delta = None\n",
    "c2_delta = None\n",
    "\n",
    "# start of additional\n",
    "# use data loaders given above\n",
    "for i, ((x_cont, x_region), (cat_labels, cont_labels)) in enumerate(pred_dl): \n",
    "    x_cont, x_region, cat_labels, cont_labels = (\n",
    "        x_cont.to(device),\n",
    "        x_region.to(device),\n",
    "        cat_labels.to(device),\n",
    "        cont_labels.to(device),\n",
    "    )\n",
    "\n",
    "    # Get outputs and calculate loss\n",
    "    cat = pred_model(x_cont)\n",
    "    pred_vals = torch.sigmoid(cat)\n",
    "    _, pred_vals2 = torch.max(torch.sigmoid(cat), 1)\n",
    "\n",
    "    attribution, conv = attr_obj.attribute( x_cont, \n",
    "                                            target = pred_vals2.item(), # so why is the predicted value the target? \n",
    "                                            #target = torch.argmax(cat_labels).item(),  # think about the correct target\n",
    "                                            return_convergence_delta = True)\n",
    "\n",
    "    # standardization of attribution values # FIXME: note, that because of this conv will not mean anything\n",
    "    test = attribution.cpu().detach().numpy()\n",
    "    if do_stnd: # perform standardization of relevance values\n",
    "        transformer = RobustScaler().fit(test.T)\n",
    "        rescaled = transformer.transform(test.T)  # replace attribution with the scaled version\n",
    "    else:\n",
    "        rescaled = test.T\n",
    "\n",
    "    if use_tp: # only use true positives\n",
    "        _, actual_cls = torch.max(torch.sigmoid(cat_labels), 1)\n",
    "        add_cond = pred_vals2.item() == actual_cls.item()\n",
    "        #print(add_cond)\n",
    "    else:\n",
    "        add_cond = True\n",
    "\n",
    "    # separate by class\n",
    "    if (pred_vals2.item() == 0) and add_cond: # class 0\n",
    "        if c0_attr is None:\n",
    "            c0_attr = rescaled\n",
    "            c0_delta = conv\n",
    "        else:\n",
    "            c0_attr = np.concatenate((c0_attr, rescaled), axis = 1)\n",
    "            c0_delta = torch.cat((c0_delta, conv), axis = 0)\n",
    "\n",
    "    elif (pred_vals2.item() == 1) and add_cond: # class 1\n",
    "        if c1_attr is None:\n",
    "            c1_attr = rescaled\n",
    "            c1_delta = conv\n",
    "        else:\n",
    "            c1_attr = np.concatenate((c1_attr, rescaled), axis = 1)\n",
    "            c1_delta = torch.cat((c1_delta, conv), axis = 0)\n",
    "    \n",
    "    elif (pred_vals2.item() == 2) and add_cond: # class 2\n",
    "        if c2_attr is None:\n",
    "            c2_attr = rescaled\n",
    "            c2_delta = conv\n",
    "        else:\n",
    "            c2_attr = np.concatenate((c2_attr, rescaled), axis = 1)\n",
    "            c2_delta = torch.cat((c2_delta, conv), axis = 0)\n",
    "\n",
    "# then transpose \n",
    "c0_attr = c0_attr.T if c0_attr is not None else None\n",
    "c1_attr = c1_attr.T if c1_attr is not None else None\n",
    "c2_attr = c2_attr.T if c2_attr is not None else None\n",
    "\n",
    "# # calculate the mean and standard deviation of the relevance \n",
    "# try to use median\n",
    "c0_mean_out = np.median(c0_attr, axis = 0) if c0_attr is not None else None\n",
    "c1_mean_out = np.median(c1_attr, axis = 0) if c1_attr is not None else None\n",
    "c2_mean_out = np.median(c2_attr, axis = 0) if c2_attr is not None else None\n",
    "\n",
    "c0_std_out = c0_attr.std(axis = 0) if c0_attr is not None else None\n",
    "c1_std_out = c1_attr.std(axis = 0) if c1_attr is not None else None\n",
    "c2_std_out = c2_attr.std(axis = 0) if c2_attr is not None else None\n",
    "\n",
    "# convert tensor to numpy array\n",
    "c0_delta    = c0_delta.detach().numpy() if c0_delta is not None else None\n",
    "c1_delta    = c1_delta.detach().numpy() if c1_delta is not None else None\n",
    "c2_delta    = c2_delta.detach().numpy() if c2_delta is not None else None\n",
    "\n",
    "# create the necessary dataframes in here\n",
    "# one for the mean and standard deviation\n",
    "# another one for the relevance values for each feature and sample\n",
    "c0_mean_rel_df = {\"FEATURE\" : pred_dl.dataset.inputs,\n",
    "                \"AVE_REL\": c0_mean_out,\n",
    "                \"STD_REL\": c0_std_out}\n",
    "c1_mean_rel_df = {\"FEATURE\" : pred_dl.dataset.inputs,\n",
    "                \"AVE_REL\": c1_mean_out,\n",
    "                \"STD_REL\": c1_std_out}\n",
    "c2_mean_rel_df = {\"FEATURE\" : pred_dl.dataset.inputs,\n",
    "                \"AVE_REL\": c2_mean_out,\n",
    "                \"STD_REL\": c2_std_out}\n",
    "\n",
    "# dataframes where feature name is the column, then the relevance values are the rows\n",
    "c0_rel_df = {}\n",
    "c1_rel_df = {}\n",
    "c2_rel_df = {}\n",
    "for i, feat in enumerate(pred_dl.dataset.inputs): # feature name as column name, then values as rows\n",
    "    c0_rel_df[feat] = c0_attr[:, i] if c0_attr is not None else None\n",
    "    c1_rel_df[feat] = c1_attr[:, i] if c1_attr is not None else None\n",
    "    c2_rel_df[feat] = c2_attr[:, i] if c2_attr is not None else None\n",
    "\n",
    "# convert to dataframes\n",
    "c0_mean_rel_df = pd.DataFrame.from_dict(c0_mean_rel_df) #sort_values(ascending = False, by = \"AVE_REL\").reset_index(drop = True)\n",
    "c1_mean_rel_df = pd.DataFrame.from_dict(c1_mean_rel_df) #sort_values(ascending = False, by = \"AVE_REL\").reset_index(drop = True)\n",
    "c2_mean_rel_df = pd.DataFrame.from_dict(c2_mean_rel_df) #sort_values(ascending = False, by = \"AVE_REL\").reset_index(drop = True)\n",
    "\n",
    "c0_rel_df = pd.DataFrame.from_dict(c0_rel_df if c0_attr is not None else [c0_rel_df])\n",
    "c1_rel_df = pd.DataFrame.from_dict(c1_rel_df if c1_attr is not None else [c1_rel_df])\n",
    "c2_rel_df = pd.DataFrame.from_dict(c2_rel_df if c2_attr is not None else [c2_rel_df])\n",
    "\n",
    "# the dataframes that we need - only up to 3 classes\n",
    "to_concat = [   c0_rel_df.median(axis = 0).to_frame().transpose(), \n",
    "                c1_rel_df.median(axis = 0).to_frame().transpose(), \n",
    "                c2_rel_df.median(axis = 0).to_frame().transpose()]\n",
    "all_median_df = pd.concat(to_concat).reset_index(drop = True)\n",
    "\n",
    "sorted_feat = arrange_features(all_median_df.columns)\n",
    "\n",
    "all_median_df = all_median_df[sorted_feat]\n",
    "\n",
    "# plot the XAI results\n",
    "all_median_df = all_median_df.transpose() # transpose\n",
    "all_median_df = all_median_df.iloc[:, ::-1] # reverse order of columns\n",
    "\n",
    "\n",
    "# all_median_df.loc[all_median_df[2] < thresh_val, 2] = 0 # class 2 - thresh 0\n",
    "# all_median_df.loc[all_median_df[1] < thresh_val, 1] = 0 # class 1 - thresh 1\n",
    "# all_median_df.loc[all_median_df[0] < thresh_val, 0] = 0\n",
    "\n",
    "## Plot the results of XAI analysis\n",
    "for i in range(num_class):\n",
    "\n",
    "    print(i)\n",
    "    fig, ax = plt.subplots(dpi = 300, figsize = (0.7, 15))\n",
    "    if do_stnd:\n",
    "        sns.heatmap(all_median_df[[i]], cmap = cmap, vmin = 0, vmax = vminmax, linewidths = 1e-3, linecolor = \"gray\",\n",
    "                    cbar_kws={'label': r\"$\\tilde{R}$\", \n",
    "                                \"shrink\" : 3,\n",
    "                                \"location\": \"bottom\", \n",
    "                                \"orientation\": \"horizontal\",\n",
    "                                \"pad\": 0.02,\n",
    "                                \"ticks\": tickmarks[tickmarks >= 0]})\n",
    "    else:\n",
    "        sns.heatmap(all_median_df[[i]], cmap = cmap, vmin = 0, vmax = vminmax, linewidths = 1e-3, linecolor = \"gray\",\n",
    "                    cbar_kws={'label': r\"$\\tilde{R}$\", \n",
    "                                \"shrink\" : 3,\n",
    "                                \"location\": \"bottom\", \n",
    "                                \"orientation\": \"horizontal\",\n",
    "                                \"pad\": 0.02,\n",
    "                                \"ticks\": tickmarks[tickmarks >= 0]})\n",
    "            \n",
    "\n",
    "    # broken lines\n",
    "    ax.hlines(borders, 0, 1, linestyle = \"-\", colors = \"k\", linewidth = 0.8)\n",
    "\n",
    "    bnds = np.array([0] + borders + [borders[-1] + 3])\n",
    "    bnds = 0.5*(bnds[1:] + bnds[:-1])\n",
    "\n",
    "    ax.set_yticks(bnds)\n",
    "    if tr_all_region_N == 0:\n",
    "        ax.set_yticklabels(xtick_lab, rotation = 0, fontsize = 10)\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top') \n",
    "    ax.set_xticklabels([hmap_labels[i]],\n",
    "                    rotation = -30, ha = \"right\")\n",
    "\n",
    "    plt_fname = out_dir/(model_name + \"_hmap_c\" + str(i) + \".png\")\n",
    "\n",
    "    plt.savefig(plt_fname, dpi = \"figure\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # histogram of the relevance values\n",
    "    plt.rc('font', size=10)\n",
    "    fig, ax = plt.subplots(dpi = 300, figsize = (2, 2))\n",
    "\n",
    "    n, bins, patches = ax.hist(all_median_df[[i]], bin_use, density=False, histtype='step',\n",
    "                            cumulative=False, label='Empirical', linewidth = 1, color = \"k\")\n",
    "    # n, bins, patches = ax.hist(all_median_df[[i]], bin_use, density=True, histtype='step',\n",
    "    #                         cumulative=True, label='Empirical', linewidth = 1, color = \"k\")\n",
    "    #ax.set_yscale(\"log\")\n",
    "    ax.set_ylim([0, ymax])\n",
    "    # ax.set_ylim([0, 1]) # for cumulative\n",
    "    # ax.vlines(0.05, 0, 1, linestyles = '--', colors = \"red\", linewidth = 0.5)\n",
    "    ax.set_xlim([-histminmax, histminmax])\n",
    "    ax.set_xlabel(r\"$\\tilde{R}$\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    plt.rc('font', size=10)\n",
    "\n",
    "    # save the histogram\n",
    "    plt_fname = out_dir/(model_name + \"_rhist_c\" + str(i) + \".png\")\n",
    "    #plt_fname = out_dir/(model_name + \"_cumu_rhist_c\" + str(i) + \".png\")\n",
    "    plt.savefig(plt_fname, dpi = \"figure\", bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CRD_UP_AVE',\n",
       " 'CRD_UP_STD',\n",
       " 'CRS_UP_AVE',\n",
       " 'CRS_UP_STD',\n",
       " 'CRM_UP_AVE',\n",
       " 'CRM_UP_STD',\n",
       " 'INV_UP_AVE',\n",
       " 'INV_UP_STD',\n",
       " 'DLT_UP_AVE',\n",
       " 'DLT_UP_STD',\n",
       " 'SED_AVE',\n",
       " 'SED_STD',\n",
       " 'SED_GRD',\n",
       " 'SRO_DG_AVE',\n",
       " 'SRO_DG_STD',\n",
       " 'IRO_DG_AVE',\n",
       " 'IRO_DG_STD',\n",
       " 'LRO_DG_AVE',\n",
       " 'LRO_DG_STD',\n",
       " 'FRE_DG_AVE',\n",
       " 'FRE_DG_STD',\n",
       " 'FRE_UP_AVE',\n",
       " 'FRE_UP_STD',\n",
       " 'EGO_UP_AVE',\n",
       " 'EGO_UP_STD',\n",
       " 'EGO_L_UP_AVE',\n",
       " 'EGO_L_UP_STD',\n",
       " 'EGO_SL_UP_AVE',\n",
       " 'EGO_SL_UP_STD',\n",
       " 'EGO_UM_UP_AVE',\n",
       " 'EGO_UM_UP_STD',\n",
       " 'EGR_DG_AVE',\n",
       " 'EGR_DG_STD',\n",
       " 'EGR_UP_AVE',\n",
       " 'EGR_UP_STD',\n",
       " 'EGR_BG_UP_AVE',\n",
       " 'EGR_BG_UP_STD',\n",
       " 'DXT_200',\n",
       " 'DXT_400',\n",
       " 'DXT_600',\n",
       " 'FDM_200',\n",
       " 'FDM_400',\n",
       " 'FDM_600',\n",
       " 'SDM_200',\n",
       " 'SDM_400',\n",
       " 'SDM_600',\n",
       " 'V_UP',\n",
       " 'V_TN',\n",
       " 'AGE']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickmarks[tickmarks >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/xai-map-model4-cls3/model4-sep100-rank1-cls3_rhist_c2.png')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2   -2.092556\n",
      "dtype: float32 2    2.140121\n",
      "dtype: float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(dpi = 100)\n",
    "if num_class == 2:\n",
    "    _ = ax.hist(all_median_df[[1]], bins = \"fd\")\n",
    "    print(all_median_df[[1]].min(), all_median_df[[1]].max())\n",
    "elif num_class == 3:\n",
    "    _ = ax.hist(all_median_df[[2]], bins = \"fd\")\n",
    "    print(all_median_df[[2]].min(), all_median_df[[2]].max())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a506c36ed4f4140d5423ef45719cc0216bbc2f05e1dadce26f453e25d80334e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('earthquakes': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
