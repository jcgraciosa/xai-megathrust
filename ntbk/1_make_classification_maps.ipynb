{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Current working directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq\n",
      "--- Reading from default.yml to build up initial hyperparameters dictionary...\n",
      "--- Checking to see if we want to override default hyperparameters with new file...\n",
      "--- Override file found! Updating with hyperparameters from default.yml...\n",
      "--- Hyperparameters/metadata set as follows (may be altered later if using EXISTING wandb sweep):\n",
      "{'entity': 'jcgraciosa',\n",
      " 'method': 'grid',\n",
      " 'parameters': {'activation_function': {'value': 'relu'},\n",
      "                'batch_normalisation': {'value': False},\n",
      "                'batch_size': {'value': 16},\n",
      "                'cat_scaling': {'value': 1},\n",
      "                'categorical_output': {'value': True},\n",
      "                'dataset': {'value': 'lin_samp_50'},\n",
      "                'dropout': {'value': 0.2},\n",
      "                'embeddings': {'value': 0},\n",
      "                'epochs': {'value': 120},\n",
      "                'exclude_file': {'value': 'xyz.csv'},\n",
      "                'hidden_layers': {'value': [500, 100]},\n",
      "                'k_folds': {'value': 5},\n",
      "                'kernel_size': {'value': [1, 1]},\n",
      "                'label_smooth': {'value': False},\n",
      "                'learning_rate': {'value': 0.01},\n",
      "                'mw_cats': {'value': [0, 0.33, 0.65, 1.0]},\n",
      "                'parameter_set': {'value': 'default'},\n",
      "                'protect_great': {'value': False},\n",
      "                'regr_scaling': {'value': 0},\n",
      "                'regression_output': {'value': False},\n",
      "                'sampling_weights': {'value': [0.33, 0.33, 0.33]},\n",
      "                'target': {'value': 'MR_GCMT'},\n",
      "                'use_SMOTE': {'value': False},\n",
      "                'use_k_fold': {'value': False},\n",
      "                'use_random_sampling': {'value': False},\n",
      "                'weight_decay': {'value': 0}},\n",
      " 'project': 'ml4szeq'}\n"
     ]
    }
   ],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %%\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "from captum.attr import LRP \n",
    "from tqdm.std import tqdm\n",
    "\n",
    "# for the LRP\n",
    "import torch.nn as nn\n",
    "from captum.attr import InputXGradient, LRP, IntegratedGradients\n",
    "from captum.attr._utils.lrp_rules import (\n",
    "    Alpha1_Beta0_Rule,\n",
    "    EpsilonRule,\n",
    "    GammaRule,\n",
    "    IdentityRule,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import vis_pkg\n",
    "import helper_pkg\n",
    "import json\n",
    "import cartopy as cartopy\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import cmocean\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['hatch.linewidth'] = 0.3\n",
    "mpl.rcParams.update({'hatch.color': 'gray'})\n",
    "\n",
    "# Terrible hack to make sure Jupyter notebooks (which use different PYTHONPATH\n",
    "# for some reason!) actually sees src/ directory so we can import from there.\n",
    "os.chdir(\"/Users/jgra0019/Documents/codes/ml4szeq/ml4szeq\")\n",
    "print(f\"--- Current working directory: {os.getcwd()}\")\n",
    "if not any([re.search(\"src$\", path) for path in sys.path]):\n",
    "    sys.path.append(str(Path.cwd() / \"src\"))\n",
    "\n",
    "import default\n",
    "from dataset import DFDataset\n",
    "from fit import Fit\n",
    "from utils import (convert_hyperparam_config_to_values, get_config,\n",
    "                   get_full_hyperparam_config, load_data)\n",
    "from model import *\n",
    "from predictor import predictions\n",
    "\n",
    "# %% GETTING HYPER-PARAMETERS\n",
    "config_override_file = get_config(\"PARAMETER_YAML_FILE\", None)\n",
    "hyperparam_config = get_full_hyperparam_config(config_override_file=config_override_file)\n",
    "print(\n",
    "    \"--- Hyperparameters/metadata set as follows (may be altered later if using EXISTING wandb sweep):\"\n",
    ")\n",
    "pprint(hyperparam_config)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping code\n",
    "All data are used in training, portion is reserved for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' settings to define '''\n",
    "\n",
    "tr_all_region_N_list = [0, 1, 2, 3, 4]\n",
    "mod_rank_list = [1, 1, 1, 1, 1] \n",
    "region_list = [\"cam\", \"sam\"]\n",
    "#region_list = [\"alu\", \"cam\", \"izu\", \"ker\", \"kur\", \"ryu\", \"sam\", \"sum\"]\n",
    "num_class = 3 # number of classes\n",
    "\n",
    "sep_dist = 250\n",
    "\n",
    "# other important config\n",
    "tr_half_use = None\n",
    "do_tr_all_region = True\n",
    "\n",
    "do_stnd = True          # perform standardization of the relevance values or not\n",
    "apply_thresh = False    # set to True if we apply threshold to heatmap values\n",
    "\n",
    "num_model = 1\n",
    "device = \"cpu\"\n",
    "\n",
    "model_dir = f\"/Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls{num_class}\"\n",
    "out_dir = Path(f\"{model_dir}/maps-sep{sep_dist}-cls{num_class}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Preparations for the map and function declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0_/fbgfh0h563nf0y4g61d675gs665gb7/T/ipykernel_66027/751418225.py:6: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = matplotlib.cm.get_cmap('Set3') # colormap used\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "preparations for the map of the predictions\n",
    "run before the loop since these don't really change\n",
    "'''\n",
    "# colormaps\n",
    "cmap = matplotlib.cm.get_cmap('Set3') # colormap used\n",
    "if num_class == 2:\n",
    "        color_list = [  matplotlib.colors.rgb2hex(cmap(6/12)),  \n",
    "                        matplotlib.colors.rgb2hex(cmap(3/12)) ]\n",
    "elif num_class == 3:\n",
    "        color_list = [  matplotlib.colors.rgb2hex(cmap(6/12)), \n",
    "                        matplotlib.colors.rgb2hex(cmap(5/12)), # 5/12 \n",
    "                        matplotlib.colors.rgb2hex(cmap(3/12)) ]\n",
    "\n",
    "# earthquake ruptures to map\n",
    "eq_file_dict = {\"alu\": [\"alu04\", \"alu06\", \"alu07\", \"alu08\", \"alu05\"],\n",
    "                \"kur\": [\"kur00\", \"kur13\", \"kur02\", \"kur20\", \"kur12\", \"kur17\", \n",
    "                        \"kur14\", \"kur01\", \"kur19\", \"kur24\", \"kur03\"],\n",
    "                \"sam\": [\"sam29\", \"sam11\", \"sam18\", \"sam24\", \"sam15\", \"sam07\", \"sam10\",\n",
    "                        \"sam22\", \"sam09\", \"sam08\", \"sam04\", \"sam03\", \"sam06\", \"sam12\",\n",
    "                        \"sam17\", \"sam26\", \"sam02\", \"sam28\", \"sam05\", \"sam19\"],\n",
    "                \"sum\": [\"sum05\", \"sum04\", \"sum08\", \"sum00\", \"sum02\",\n",
    "                        \"sum03\", \"sum06\"],\n",
    "                \"cam\": [\"cam03\", \"cam10\", \"cam07\", \"cam11\", \"cam01\", \"cam09\",\n",
    "                        \"cam17\", \"cam12\", \"cam04\", \"cam00\", \"cam15\"],\n",
    "                \"ker\": [\"ker02\", \"ker01\", \"ker00\"],\n",
    "                \"ryu\": [\"ryu00\", \"ryu01\", \"ryu02\"],\n",
    "                \"izu\": []               \n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform predictions and put in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/model0\n",
      "Output directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/maps-sep250-cls3\n",
      "Dropping any columns which have fewer than  90% (531.9) values\n",
      "Dropping any rows which are missing *any* input variables\n",
      "Dropped 33 columns, and 35 rows.\n",
      "Executing power transformer ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (556). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/model1\n",
      "Output directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/maps-sep250-cls3\n",
      "Dropping any columns which have fewer than  90% (531.9) values\n",
      "Dropping any rows which are missing *any* input variables\n",
      "Dropped 33 columns, and 35 rows.\n",
      "Executing power transformer ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (556). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/model2\n",
      "Output directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/maps-sep250-cls3\n",
      "Dropping any columns which have fewer than  90% (531.9) values\n",
      "Dropping any rows which are missing *any* input variables\n",
      "Dropped 33 columns, and 35 rows.\n",
      "Executing power transformer ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (556). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/model3\n",
      "Output directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/maps-sep250-cls3\n",
      "Dropping any columns which have fewer than  90% (531.9) values\n",
      "Dropping any rows which are missing *any* input variables\n",
      "Dropped 33 columns, and 35 rows.\n",
      "Executing power transformer ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (556). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/model4\n",
      "Output directory: /Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/maps-sep250-cls3\n",
      "Dropping any columns which have fewer than  90% (531.9) values\n",
      "Dropping any rows which are missing *any* input variables\n",
      "Dropped 33 columns, and 35 rows.\n",
      "Executing power transformer ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:2663: UserWarning: n_quantiles (1000) is greater than the total number of samples (556). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# perform predictions\n",
    "\n",
    "random.seed(43) # set the random seed \n",
    "pred_dict = {}\n",
    "\n",
    "for mod_rank, tr_all_region_N in zip(mod_rank_list, tr_all_region_N_list):\n",
    "    \n",
    "    hparam_file = f\"{model_dir}/model{tr_all_region_N}/model{tr_all_region_N}-sep{sep_dist}-cls{num_class}-top10.json\"\n",
    "    hparam_file = Path(hparam_file)\n",
    "\n",
    "    with open(hparam_file) as json_data_file:\n",
    "        hparam = json.load(json_data_file)\n",
    "\n",
    "    hparam_sset = hparam\n",
    "\n",
    "    hparam_sset = hparam_sset[str(mod_rank)]\n",
    "\n",
    "    epoch_use = hparam_sset[\"epoch_use\"]\n",
    "    folder_use = hparam_sset[\"folder\"]\n",
    "    \n",
    "    mod_dir = Path(f\"{model_dir}/model{tr_all_region_N}\")\n",
    "\n",
    "    print(f\"Model directory: {mod_dir}\")\n",
    "    print(f\"Output directory: {out_dir}\")\n",
    "\n",
    "    ''' MACHINE LEARNING STUFF HERE '''\n",
    "\n",
    "    # set-up hyperparameters - override values in default.yml \n",
    "    hyperparam_config[\"parameters\"][\"exclude_file\"][\"value\"] = \"xyz.csv\" \n",
    "    hyperparam_config[\"parameters\"][\"dropout\"][\"value\"] = float(hparam_sset[\"dropout\"])\n",
    "    hyperparam_config[\"parameters\"][\"hidden_layers\"][\"value\"] = hparam_sset['hidden_layers']\n",
    "    hyperparam_config[\"parameters\"][\"batch_size\"][\"value\"] = hparam_sset[\"batch_sz\"]\n",
    "    hyperparam_config[\"parameters\"][\"learning_rate\"][\"value\"] = float(hparam_sset[\"lr\"])\n",
    "\n",
    "    params, _ = convert_hyperparam_config_to_values(hyperparam_config) # convert here to include whatever were overriden\n",
    "    data_suffix = params.get(\"dataset\", \"16k\")  # which dataset you want as input\n",
    "    data_folder = default.ROOT_DATA_DIRECTORY / data_suffix\n",
    "    use_cache = False\n",
    "\n",
    "    preprocessor = load_data(\n",
    "            data_folder=data_folder,\n",
    "            exclude_file=\"xyz.csv\",\n",
    "            target=params[\"target\"],\n",
    "            cats=params[\"mw_cats\"],\n",
    "            rand_seed = None, # for sampling with replacement \n",
    "            kernel_size=params[\"kernel_size\"],\n",
    "            use_cache=use_cache,\n",
    "            rd_exclude = False if do_tr_all_region else True,\n",
    "            protect_great=params[\"protect_great\"],\n",
    "            tr_half_use = tr_half_use,\n",
    "            sep_dist = sep_dist,\n",
    "            tr_all_region = False if do_tr_all_region else None,\n",
    "            tr_all_region_N = tr_all_region_N\n",
    "        )\n",
    "\n",
    "        # Define arguments to be passed into our testing loop function.\n",
    "    full_pred_kwargs = dict(\n",
    "        df=preprocessor.dataframe,\n",
    "        inputs=preprocessor.inputs,\n",
    "        hyperparam_config=hyperparam_config,\n",
    "        model_name_add=None,\n",
    "        use_wandb=False,\n",
    "    )\n",
    "\n",
    "    # create pred_ds \n",
    "    pred_ds = DFDataset(dataframe = preprocessor.dataframe, \n",
    "                        inputs=preprocessor.inputs, \n",
    "                        target=preprocessor.target, \n",
    "                        force_cats = 5)\n",
    "    pred_dl = torch.utils.data.DataLoader(pred_ds, batch_size = 1, shuffle=False)\n",
    "    test_df = copy.deepcopy(pred_dl.dataset.dataframe)\n",
    "\n",
    "    out_df = {}\n",
    "    out_df[\"S_AVE\"] = test_df[\"S_AVE\"]\n",
    "    out_df[\"LON_AVE\"] = test_df[\"LON_AVE\"]\n",
    "    out_df[\"LAT_AVE\"] = test_df[\"LAT_AVE\"]\n",
    "    out_df[\"MR_ISC\"] = test_df[\"MR_ISC\"]\n",
    "    out_df[\"MR_GCMT\"] = test_df[\"MR_GCMT\"]\n",
    "    out_df[\"MW_CAT\"] = test_df[\"MW_CAT\"]\n",
    "    out_df[\"REGION\"] = test_df[\"REGION_NAME\"]\n",
    "\n",
    "    ################## PERFORM PREDICTION \n",
    "    model_fname = \"epoch-\" + str(epoch_use) + \".pt\"\n",
    "    idx = 0\n",
    "\n",
    "    model_path = mod_dir/hparam_sset[\"folder\"]\n",
    "        \n",
    "    pred_obj = Fit(fit_on_init = False, **full_pred_kwargs, force_cats = 0) # initialize lang pirmi\n",
    "    pred_model = copy.deepcopy(pred_obj.model)\n",
    "\n",
    "    #loop through all \n",
    "    pred_model.load_state_dict(torch.load(model_path))\n",
    "    pred_model.to(device)\n",
    "\n",
    "    pred_model.eval()\n",
    "    preds = np.zeros([len(pred_dl.dataset.dataframe), num_class])\n",
    "    class_preds = np.zeros([len(pred_dl.dataset.dataframe)])\n",
    "\n",
    "    # Loop through test data\n",
    "    with torch.no_grad():\n",
    "        for i, ((x_cont, x_region), (cat_labels, cont_labels)) in enumerate(pred_dl): \n",
    "            x_cont, x_region, cat_labels, cont_labels = (\n",
    "                x_cont.to(device),\n",
    "                x_region.to(device),\n",
    "                cat_labels.to(device),\n",
    "                cont_labels.to(device),\n",
    "            )\n",
    "\n",
    "            # Get outputs and calculate loss\n",
    "            cat = pred_model(x_cont)\n",
    "            pred_vals = torch.sigmoid(cat)\n",
    "            _, pred_vals2 = torch.max(torch.sigmoid(cat), 1)\n",
    "            #print(cat)\n",
    "\n",
    "            preds[i] = pred_vals\n",
    "            class_preds[i] = pred_vals2\n",
    "\n",
    "        # save to dataframe for evaluation\n",
    "        for i in range(num_class):\n",
    "            out_df[\"MDL_\"+ str(idx) + \"_CLS_\" + str(i)] = preds[:, i]\n",
    "\n",
    "    out_df[\"MDL_\" + str(idx) + \"_PRED\"] = class_preds\n",
    "    idx += 1\n",
    "\n",
    "    out_df = pd.DataFrame(out_df)\n",
    "    out_df[\"CLASS_PRED\"] = class_preds\n",
    "\n",
    "    pred_dict[tr_all_region_N] = out_df\n",
    "\n",
    "    ########## prepare the results\n",
    "    # num_model = 1\n",
    "\n",
    "    # col_list =[\"MDL_\" + str(x) + \"_CLS_0\" for x in range(num_model)]\n",
    "    # cls0_df = out_df[ [\"S_AVE\"] + col_list]\n",
    "    # cls0_df = cls0_df.assign(MEAN=cls0_df[col_list].mean(axis = 1))\n",
    "    # cls0_df = cls0_df.assign(STD=cls0_df[col_list].std(axis = 1))\n",
    "\n",
    "    # col_list = [\"MDL_\" + str(x) + \"_CLS_1\" for x in range(num_model)]\n",
    "    # cls1_df = out_df[[\"S_AVE\"] + col_list]\n",
    "    # cls1_df = cls1_df.assign(MEAN=cls1_df[col_list].mean(axis = 1))\n",
    "    # cls1_df = cls1_df.assign(STD=cls1_df[col_list].std(axis = 1))\n",
    "\n",
    "    # # col_list = [\"MDL_\" + str(x) + \"_CLS_2\" for x in range(num_model)]\n",
    "    # # cls2_df = out_df[[\"S_AVE\"] + col_list]\n",
    "    # # cls2_df = cls2_df.assign(MEAN=cls2_df[col_list].mean(axis = 1))\n",
    "    # # cls2_df = cls2_df.assign(STD=cls2_df[col_list].std(axis = 1))\n",
    "\n",
    "    # col_list = [\"MDL_\" + str(x) + \"_PRED\" for x in range(num_model)]\n",
    "    # pred_df = out_df[[\"S_AVE\"] + col_list]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Loop through all the models, perform predictions, and map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dir = Path(\"/Users/jgra0019/Documents/codes/region-paths\")\n",
    "map_set_fname = \"/Users/jgra0019/Documents/codes/globdat-paths/map_setting_conf.json\"\n",
    "\n",
    "with open(map_set_fname) as data_file:\n",
    "    #print(map_set_fname)\n",
    "    all_map_setting = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  /Users/jgra0019/Documents/phd_research/data/topography/cam_region.grd\n",
      "['Conventions', 'history', 'GMT_version', 'node_offset']\n",
      "['lon', 'lat']\n",
      "['lon', 'lat', 'z']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/cartopy/mpl/gridliner.py:451: UserWarning: The .xlabels_top attribute is deprecated. Please use .top_labels to toggle visibility instead.\n",
      "  warnings.warn('The .xlabels_top attribute is deprecated. Please '\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/cartopy/mpl/gridliner.py:487: UserWarning: The .ylabels_right attribute is deprecated. Please use .right_labels to toggle visibility instead.\n",
      "  warnings.warn('The .ylabels_right attribute is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/maps-sep250-cls3/cam-sep250.png\n",
      "Reading  /Users/jgra0019/Documents/phd_research/data/topography/sam_region.grd\n",
      "['Conventions', 'history', 'GMT_version', 'node_offset']\n",
      "['lon', 'lat']\n",
      "['lon', 'lat', 'z']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/cartopy/mpl/gridliner.py:451: UserWarning: The .xlabels_top attribute is deprecated. Please use .top_labels to toggle visibility instead.\n",
      "  warnings.warn('The .xlabels_top attribute is deprecated. Please '\n",
      "/Users/jgra0019/mambaforge/envs/earthquakes/lib/python3.11/site-packages/cartopy/mpl/gridliner.py:487: UserWarning: The .ylabels_right attribute is deprecated. Please use .right_labels to toggle visibility instead.\n",
      "  warnings.warn('The .ylabels_right attribute is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jgra0019/Documents/codes/ml4szeq/ml4szeq/out/models/all-used-training-wbal-cls3/maps-sep250-cls3/sam-sep250.png\n"
     ]
    }
   ],
   "source": [
    "dpi = 300\n",
    "\n",
    "for region in region_list:\n",
    "\n",
    "    # get all the data for current region\n",
    "    region_df = []\n",
    "\n",
    "    for tr_all_region_N in tr_all_region_N_list:\n",
    "\n",
    "        df = pred_dict[tr_all_region_N]\n",
    "        sset = df[df[\"REGION\"] == region]\n",
    "        region_df.append(sset)\n",
    "\n",
    "    region_df = pd.concat(region_df).reset_index(drop = True)\n",
    "\n",
    "    # config file\n",
    "    conf_fname = conf_dir/(region + \"_conf.json\")\n",
    "    with open(conf_fname) as json_data_file:\n",
    "        conf = json.load(json_data_file)\n",
    "\n",
    "    map_setting = all_map_setting[region]\n",
    "    root_dir = Path(conf[\"mac_root\"])\n",
    "    other_settings = conf[\"conf\"]\n",
    "    conf = conf[\"path\"]\n",
    "\n",
    "    ##########################\n",
    "    # Map settings\n",
    "    ##########################\n",
    "\n",
    "    # convert range of longitude to -180 to 180 if alu\n",
    "    is_alu = False\n",
    "    if any(re.findall(r'alu', region, re.IGNORECASE)):\n",
    "        is_alu = True\n",
    "    is_ker = False\n",
    "    if any(re.findall(r'ker', region, re.IGNORECASE)):\n",
    "        is_ker = True\n",
    "\n",
    "    # read trench data\n",
    "    trench_plt = pd.read_csv(root_dir/conf[\"trench_full\"], sep=',', header = None)\n",
    "    trench_plt.columns = ['LON', 'LAT']\n",
    "\n",
    "    # text labels \n",
    "    label_fname = root_dir/conf[\"map_label\"]\n",
    "    label_df = pd.read_csv(label_fname, header = 'infer')\n",
    "    label_df = label_df[label_df[\"TEXT\"] != \"Reference (0 km)\"] # remove the reference - replace with a red marker\n",
    "    label_df = label_df[label_df[\"IS_EQ\"] != True] # remove earthquake labels\n",
    "    label_df.loc[label_df[\"COLOR\"] == 'yellow', \"COLOR\"] = 'firebrick'\n",
    "\n",
    "    # earthquake slip files\n",
    "    fdir = root_dir/conf[\"slip_line2\"]\n",
    "\n",
    "    eq_files = eq_file_dict[region]\n",
    "\n",
    "    eq_outlines = {}\n",
    "    # iterate opening\n",
    "    cnt = 0\n",
    "    for subdir, dirs, files in os.walk(fdir):\n",
    "        for file in files:\n",
    "            file_id = file.replace(\"_sqk_\", \"\").replace(\"_rup.csv\", \"\")\n",
    "            if file_id in eq_files:\n",
    "                fname = fdir/file\n",
    "                eq_df = pd.read_csv(fname, sep = ',', comment = \"#\")\n",
    "                #print(eq_df)\n",
    "                #print(fname)\n",
    "                if eq_df.shape[1] > 2:\n",
    "                    eq_df = eq_df[['LON', 'LAT', 'COUNT']]\n",
    "                    eq_df = eq_df.sort_values(by = 'COUNT')\n",
    "\n",
    "                eq_outlines[cnt] = eq_df\n",
    "                cnt += 1\n",
    "\n",
    "    # other settings\n",
    "    arr_sz = map_setting[\"trench_arr_sz\"] # trench arrow size\n",
    "    rel_pos = map_setting[\"label_rel_pos\"] # for the label\n",
    "    contour_fsize = map_setting[\"contour_fnt_sz\"]  # contour label font size\n",
    "    lwidth = map_setting[\"contour_lwidth\"] # contour line width\n",
    "    label_x = map_setting[\"region_lab_x\"] # region label x\n",
    "    label_y = map_setting[\"region_lab_y\"] # region label y\n",
    "    as_line = map_setting[\"eq_as_line\"]\n",
    "\n",
    "    # generate grid data since you need to do some revisions\n",
    "    ''' Make sure these are correct!!! Especially ds value. '''\n",
    "    n_max = 300 # in km in direction of downgoing plate\n",
    "    n_min = -300 # in km in direction of upper plate\n",
    "    dn = 300 # step in the n-axis\n",
    "    ds = 50 # step in the s-axis \n",
    "\n",
    "    # Open the topo grid file\n",
    "    # process topo grid file\n",
    "    topo_fname = root_dir/conf['topo_grd']\n",
    "    lon_topo, lat_topo, elev, topo_hs = vis_pkg.process_topo(topo_fname, set_lon360 = True if is_alu or is_ker else False)\n",
    "\n",
    "    # since i don't want to write new code, make a work-around\n",
    "    feat_df = pd.read_csv(root_dir/conf[\"dilat\"], header =None, sep = \"\\t\")\n",
    "    feat_df = feat_df.dropna()\n",
    "    feat_df = feat_df.reset_index(drop=True)\n",
    "    feat_df.columns = [\"LON\", \"LAT\", \"VAL\"]\n",
    "    feat_df[\"LON\"] = feat_df[\"LON\"]%360 # make sure to convert \n",
    "\n",
    "\n",
    "    n_ax, s_ax, lon_grid, lat_grid = helper_pkg.make_grid(root_dir/conf[\"grid_dir_\" + str(ds)], \n",
    "                                                        n_max = n_max, n_min = n_min, \n",
    "                                                        dn = dn, ds = ds, \n",
    "                                                        ncdf_fname = None)\n",
    "    lon_grid = lon_grid%360\n",
    "\n",
    "    workaround_df = helper_pkg.map_data_to_grid(s_ax = s_ax, n_ax = n_ax, \n",
    "                                        lon_grid = lon_grid, lat_grid = lat_grid, \n",
    "                                        dep_df = None,  \n",
    "                                        in_data_df = feat_df, \n",
    "                                        mode = 0, \n",
    "                                        rm_unmapped = True)\n",
    "\n",
    "    # setting for the sort_by - for the PatchCollection\n",
    "    if region in [\"alu\"]:\n",
    "        sort_by = 0\n",
    "    elif region in [\"sam\", \"kur\", \"sum\"]:\n",
    "        sort_by = 1\n",
    "    else:\n",
    "        sort_by = 1\n",
    "\n",
    "    ######### Create the map\n",
    "    # just run every iteration since out_df is here - actually just need to run during first iteration\n",
    "    workaround_df = workaround_df[workaround_df[\"S_AVE\"] <= region_df[\"S_AVE\"].max()]  \n",
    "    #lon_grid = lon_grid%360 # convert to 0 - 360\n",
    "\n",
    "    for s in region_df[\"S_AVE\"].unique():\n",
    "        val_neg = workaround_df[(workaround_df[\"S_AVE\"] == s) & (workaround_df[\"N_AVE\"] < 0)] \n",
    "        val_pos = workaround_df[(workaround_df[\"S_AVE\"] == s) & (workaround_df[\"N_AVE\"] > 0)] \n",
    "        \n",
    "        region_df.loc[region_df[\"S_AVE\"] == s, \"LON_NEG\"] = val_neg[\"LON_AVE\"].max()\n",
    "        region_df.loc[region_df[\"S_AVE\"] == s, \"LAT_NEG\"] = val_neg[\"LAT_AVE\"].max()\n",
    "\n",
    "        region_df.loc[region_df[\"S_AVE\"] == s, \"LON_POS\"] = val_pos[\"LON_AVE\"].max()\n",
    "        region_df.loc[region_df[\"S_AVE\"] == s, \"LAT_POS\"] = val_pos[\"LAT_AVE\"].max()\n",
    "\n",
    "    region_df = region_df.sort_values(by = \"S_AVE\", ascending = True)\n",
    "\n",
    "    ########## Assign colors\n",
    "    for category in region_df[\"MW_CAT\"].unique(): # expected class\n",
    "        region_df.loc[region_df[\"MW_CAT\"] == category, \"EX_CLR\"] = color_list[category]\n",
    "\n",
    "    for category in region_df[\"CLASS_PRED\"].unique(): # predicted class\n",
    "        region_df.loc[region_df[\"CLASS_PRED\"] == category, \"ML_CLR\"] = color_list[int(category)]\n",
    "    \n",
    "    ''' create the maps containing the predictions '''\n",
    "    mapper = vis_pkg.Mapper(dpi = dpi,\n",
    "                            data = None,\n",
    "                            topo = {'LON': lon_topo, 'LAT': lat_topo, 'VAL': topo_hs},\n",
    "                            extent = other_settings[\"map_extent\"],\n",
    "                            dim_inch = other_settings[\"map_wh_inch\"],\n",
    "                            is_alu = is_alu,\n",
    "                            is_ker = is_ker\n",
    "                            )\n",
    "    mapper.create_basemap()\n",
    "    mapper.add_trench_line(trench_df = trench_plt, linewidth = 0.7)\n",
    "    mapper.add_trench_marker(trench_fname = root_dir/conf[\"trench_used\"],\n",
    "                            trench_arr_sz = arr_sz\n",
    "                            )\n",
    "    mapper.add_topo()\n",
    "    # plot for the predicted class\n",
    "    mapper.add_ml_1d_data_poly( df = region_df, \n",
    "                                lon_col = \"LON_POS\", lat_col = \"LAT_POS\", \n",
    "                                clr_col = \"ML_CLR\", sort_by = sort_by, alpha = 1)\n",
    "    # plot for the expected class\n",
    "    mapper.add_ml_1d_data_poly( df = region_df, \n",
    "                                lon_col = \"LON_NEG\", lat_col = \"LAT_NEG\", \n",
    "                                clr_col = \"EX_CLR\", sort_by = sort_by, alpha = 1, \n",
    "                                poly_edge_width = 0.1)\n",
    "    mapper.add_slip_outlines(eq_outlines_dict = {'OUTLINE': eq_outlines, \n",
    "                                                'COLOR': \"mediumblue\", \n",
    "                                                'AS_LINE':True})\n",
    "    mapper.add_map_labels(label_dict = {'LAB': label_df, 'REL_POS': rel_pos})\n",
    "\n",
    "    # the legend\n",
    "    leg_lab = [ r\"$M_w < 8.0$\", \n",
    "                r\"$M_w \\geq 8.0$\"]\n",
    "\n",
    "    patch_list = [mpatches.Patch(color = x, ec = \"k\", linewidth = 0.2, label = \"C\" + str(i) + \": \" + lab) for i, (lab, x) in enumerate(zip(leg_lab, color_list))]\n",
    "\n",
    "    if region in [\"kur\"]:\n",
    "        legend = mapper.ax.legend(handles=patch_list, fontsize = 7, loc = \"upper left\")\n",
    "        legend.get_frame().set_linewidth(0.2)\n",
    "\n",
    "    # save the files\n",
    "    out_dir.mkdir(parents=True, exist_ok=True) \n",
    "    map_fname = out_dir/(region + \"-sep\" + str(sep_dist) + \".png\")\n",
    "    print(map_fname)\n",
    "    plt.savefig(map_fname, dpi = \"figure\", bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-91, -56, -47, 11.5]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_settings[\"map_extent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24467     280.746725\n",
       "24468     280.746725\n",
       "24741     280.746725\n",
       "24742     280.746725\n",
       "24743     280.746725\n",
       "             ...    \n",
       "164534    285.973771\n",
       "164535    285.973771\n",
       "164536    285.973771\n",
       "164537    285.973771\n",
       "164538    285.973771\n",
       "Name: LON_AVE, Length: 22875, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workaround_df[\"LON_AVE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-90.975, -90.925, -90.875, -90.825, -90.775, -90.725, -90.675,\n",
       "       -90.625, -90.575, -90.525, -90.475, -90.425, -90.375, -90.325,\n",
       "       -90.275, -90.225, -90.175, -90.125, -90.075, -90.025, -89.975,\n",
       "       -89.925, -89.875, -89.825, -89.775, -89.725, -89.675, -89.625,\n",
       "       -89.575, -89.525, -89.475, -89.425, -89.375, -89.325, -89.275,\n",
       "       -89.225, -89.175, -89.125, -89.075, -89.025, -88.975, -88.925,\n",
       "       -88.875, -88.825, -88.775, -88.725, -88.675, -88.625, -88.575,\n",
       "       -88.525, -88.475, -88.425, -88.375, -88.325, -88.275, -88.225,\n",
       "       -88.175, -88.125, -88.075, -88.025, -87.975, -87.925, -87.875,\n",
       "       -87.825, -87.775, -87.725, -87.675, -87.625, -87.575, -87.525,\n",
       "       -87.475, -87.425, -87.375, -87.325, -87.275, -87.225, -87.175,\n",
       "       -87.125, -87.075, -87.025, -86.975, -86.925, -86.875, -86.825,\n",
       "       -86.775, -86.725, -86.675, -86.625, -86.575, -86.525, -86.475,\n",
       "       -86.425, -86.375, -86.325, -86.275, -86.225, -86.175, -86.125,\n",
       "       -86.075, -86.025, -85.975, -85.925, -85.875, -85.825, -85.775,\n",
       "       -85.725, -85.675, -85.625, -85.575, -85.525, -85.475, -85.425,\n",
       "       -85.375, -85.325, -85.275, -85.225, -85.175, -85.125, -85.075,\n",
       "       -85.025, -84.975, -84.925, -84.875, -84.825, -84.775, -84.725,\n",
       "       -84.675, -84.625, -84.575, -84.525, -84.475, -84.425, -84.375,\n",
       "       -84.325, -84.275, -84.225, -84.175, -84.125, -84.075, -84.025,\n",
       "       -83.975, -83.925, -83.875, -83.825, -83.775, -83.725, -83.675,\n",
       "       -83.625, -83.575, -83.525, -83.475, -83.425, -83.375, -83.325,\n",
       "       -83.275, -83.225, -83.175, -83.125, -83.075, -83.025, -82.975,\n",
       "       -82.925, -82.875, -82.825, -82.775, -82.725, -82.675, -82.625,\n",
       "       -82.575, -82.525, -82.475, -82.425, -82.375, -82.325, -82.275,\n",
       "       -82.225, -82.175, -82.125, -82.075, -82.025, -81.975, -81.925,\n",
       "       -81.875, -81.825, -81.775, -81.725, -81.675, -81.625, -81.575,\n",
       "       -81.525, -81.475, -81.425, -81.375, -81.325, -81.275, -81.225,\n",
       "       -81.175, -81.125, -81.075, -81.025, -80.975, -80.925, -80.875,\n",
       "       -80.825, -80.775, -80.725, -80.675, -80.625, -80.575, -80.525,\n",
       "       -80.475, -80.425, -80.375, -80.325, -80.275, -80.225, -80.175,\n",
       "       -80.125, -80.075, -80.025, -79.975, -79.925, -79.875, -79.825,\n",
       "       -79.775, -79.725, -79.675, -79.625, -79.575, -79.525, -79.475,\n",
       "       -79.425, -79.375, -79.325, -79.275, -79.225, -79.175, -79.125,\n",
       "       -79.075, -79.025, -78.975, -78.925, -78.875, -78.825, -78.775,\n",
       "       -78.725, -78.675, -78.625, -78.575, -78.525, -78.475, -78.425,\n",
       "       -78.375, -78.325, -78.275, -78.225, -78.175, -78.125, -78.075,\n",
       "       -78.025, -77.975, -77.925, -77.875, -77.825, -77.775, -77.725,\n",
       "       -77.675, -77.625, -77.575, -77.525, -77.475, -77.425, -77.375,\n",
       "       -77.325, -77.275, -77.225, -77.175, -77.125, -77.075, -77.025,\n",
       "       -76.975, -76.925, -76.875, -76.825, -76.775, -76.725, -76.675,\n",
       "       -76.625, -76.575, -76.525, -76.475, -76.425, -76.375, -76.325,\n",
       "       -76.275, -76.225, -76.175, -76.125, -76.075, -76.025, -75.975,\n",
       "       -75.925, -75.875, -75.825, -75.775, -75.725, -75.675, -75.625,\n",
       "       -75.575, -75.525, -75.475, -75.425, -75.375, -75.325, -75.275,\n",
       "       -75.225, -75.175, -75.125, -75.075, -75.025, -74.975, -74.925,\n",
       "       -74.875, -74.825, -74.775, -74.725, -74.675, -74.625, -74.575,\n",
       "       -74.525, -74.475, -74.425, -74.375, -74.325, -74.275, -74.225,\n",
       "       -74.175, -74.125, -74.075, -74.025, -73.975, -73.925, -73.875,\n",
       "       -73.825, -73.775, -73.725, -73.675, -73.625, -73.575, -73.525,\n",
       "       -73.475, -73.425, -73.375, -73.325, -73.275, -73.225, -73.175,\n",
       "       -73.125, -73.075, -73.025, -72.975, -72.925, -72.875, -72.825,\n",
       "       -72.775, -72.725, -72.675, -72.625, -72.575, -72.525, -72.475,\n",
       "       -72.425, -72.375, -72.325, -72.275, -72.225, -72.175, -72.125,\n",
       "       -72.075, -72.025, -71.975, -71.925, -71.875, -71.825, -71.775,\n",
       "       -71.725, -71.675, -71.625, -71.575, -71.525, -71.475, -71.425,\n",
       "       -71.375, -71.325, -71.275, -71.225, -71.175, -71.125, -71.075,\n",
       "       -71.025, -70.975, -70.925, -70.875, -70.825, -70.775, -70.725,\n",
       "       -70.675, -70.625, -70.575, -70.525, -70.475, -70.425, -70.375,\n",
       "       -70.325, -70.275, -70.225, -70.175, -70.125, -70.075, -70.025,\n",
       "       -69.975, -69.925, -69.875, -69.825, -69.775, -69.725, -69.675,\n",
       "       -69.625, -69.575, -69.525, -69.475, -69.425, -69.375, -69.325,\n",
       "       -69.275, -69.225, -69.175, -69.125, -69.075, -69.025, -68.975,\n",
       "       -68.925, -68.875, -68.825, -68.775, -68.725, -68.675, -68.625,\n",
       "       -68.575, -68.525, -68.475, -68.425, -68.375, -68.325, -68.275,\n",
       "       -68.225, -68.175, -68.125, -68.075, -68.025, -67.975, -67.925,\n",
       "       -67.875, -67.825, -67.775, -67.725, -67.675, -67.625, -67.575,\n",
       "       -67.525, -67.475, -67.425, -67.375, -67.325, -67.275, -67.225,\n",
       "       -67.175, -67.125, -67.075, -67.025, -66.975, -66.925, -66.875,\n",
       "       -66.825, -66.775, -66.725, -66.675, -66.625, -66.575, -66.525,\n",
       "       -66.475, -66.425, -66.375, -66.325, -66.275, -66.225, -66.175,\n",
       "       -66.125, -66.075, -66.025, -65.975, -65.925, -65.875, -65.825,\n",
       "       -65.775, -65.725, -65.675, -65.625, -65.575, -65.525, -65.475,\n",
       "       -65.425, -65.375, -65.325, -65.275, -65.225, -65.175, -65.125,\n",
       "       -65.075, -65.025, -64.975, -64.925, -64.875, -64.825, -64.775,\n",
       "       -64.725, -64.675, -64.625, -64.575, -64.525, -64.475, -64.425,\n",
       "       -64.375, -64.325, -64.275, -64.225, -64.175, -64.125, -64.075,\n",
       "       -64.025, -63.975, -63.925, -63.875, -63.825, -63.775, -63.725,\n",
       "       -63.675, -63.625, -63.575, -63.525, -63.475, -63.425, -63.375,\n",
       "       -63.325, -63.275, -63.225, -63.175, -63.125, -63.075, -63.025,\n",
       "       -62.975, -62.925, -62.875, -62.825, -62.775, -62.725, -62.675,\n",
       "       -62.625, -62.575, -62.525, -62.475, -62.425, -62.375, -62.325,\n",
       "       -62.275, -62.225, -62.175, -62.125, -62.075, -62.025, -61.975,\n",
       "       -61.925, -61.875, -61.825, -61.775, -61.725, -61.675, -61.625,\n",
       "       -61.575, -61.525, -61.475, -61.425, -61.375, -61.325, -61.275,\n",
       "       -61.225, -61.175, -61.125, -61.075, -61.025, -60.975, -60.925,\n",
       "       -60.875, -60.825, -60.775, -60.725, -60.675, -60.625, -60.575,\n",
       "       -60.525, -60.475, -60.425, -60.375, -60.325, -60.275, -60.225,\n",
       "       -60.175, -60.125, -60.075, -60.025, -59.975, -59.925, -59.875,\n",
       "       -59.825, -59.775, -59.725, -59.675, -59.625, -59.575, -59.525,\n",
       "       -59.475, -59.425, -59.375, -59.325, -59.275, -59.225, -59.175,\n",
       "       -59.125, -59.075, -59.025, -58.975, -58.925, -58.875, -58.825,\n",
       "       -58.775, -58.725, -58.675, -58.625, -58.575, -58.525, -58.475,\n",
       "       -58.425, -58.375, -58.325, -58.275, -58.225, -58.175, -58.125,\n",
       "       -58.075, -58.025, -57.975, -57.925, -57.875, -57.825, -57.775,\n",
       "       -57.725, -57.675, -57.625, -57.575, -57.525, -57.475, -57.425,\n",
       "       -57.375, -57.325, -57.275, -57.225, -57.175, -57.125, -57.075,\n",
       "       -57.025, -56.975, -56.925, -56.875, -56.825, -56.775, -56.725,\n",
       "       -56.675, -56.625, -56.575, -56.525, -56.475, -56.425, -56.375,\n",
       "       -56.325, -56.275, -56.225, -56.175, -56.125, -56.075, -56.025])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lon_topo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a506c36ed4f4140d5423ef45719cc0216bbc2f05e1dadce26f453e25d80334e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('earthquakes': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
